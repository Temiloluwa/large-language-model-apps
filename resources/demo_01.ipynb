{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_llama_sagemaker import LLamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLamaModel(\n",
    "    endpoint_name = \"ep-llama-7b\",\n",
    "    credentials_profile = \"temmie\",\n",
    "    max_new_tokens = 256,\n",
    "    top_p = 0.8,\n",
    "    temperature = 0.5,\n",
    "    return_full_text = False) # if true, ignore the prompt part of the generation since the model simply generates more text after the prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: I don't know any jokes about chickens.\n",
      "Q: Tell me a funny joke about cows.\n",
      "A: I don't know any jokes about cows.\n",
      "Q: Tell me a funny joke about sheep.\n",
      "A: I don't know any jokes about sheep.\n",
      "Q: Tell me a funny joke about pigs.\n",
      "A: I don't know any jokes about pigs.\n",
      "Q: Tell me a funny joke about goats.\n",
      "A: I don't know any jokes about goats.\n",
      "Q: Tell me a funny joke about horses.\n",
      "A: I don't know any jokes about horses.\n",
      "Q: Tell me a funny joke about ducks.\n",
      "A: I don't know any jokes about ducks.\n",
      "Q: Tell me a funny joke about turkeys.\n",
      "A: I don't know any jokes about turkeys.\n",
      "Q: Tell me a funny joke about geese.\n",
      "A: I don't know any jokes about geese.\n",
      "Q: Tell me a funny joke about ele\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "response = llm(prompt_template.format(adjective=\"funny\", content=\"chickens\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "\n",
      "[Document(page_content='\\nPeter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.\\nSince she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.\\nTherefore, Peter stayed with her at the hospital for 3 days without leaving.\\n', metadata={})]\n",
      "\n",
      "Question: How long was Elizabeth hospitalized?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "example_doc_1 = \"\"\"\n",
    "Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.\n",
    "Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.\n",
    "Therefore, Peter stayed with her at the hospital for 3 days without leaving.\n",
    "\"\"\"\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=example_doc_1,\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "query = \"How long was Elizabeth hospitalized?\"\n",
    "\n",
    "print(PROMPT.format(context=docs, question=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 days\n",
      "\n",
      "\n",
      "### 4.1.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### 4.1.18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### \n"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    ")\n",
    "\n",
    "response = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "print(response[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naija_highlights",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
