{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from queryverse.llm import OpenAILLM\n",
    "from queryverse.prompter import SystemPrompter, UserPrompter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemPrompter(\"\"\"\n",
    "        You are a fluent German speaker that is great at following instructions. \n",
    "    \"\"\")\n",
    "\n",
    "user_prompt = UserPrompter(\"\"\"\n",
    "    Given a German word, supply the following\n",
    "    1. A explanation of the word in english in very simple terms like to a child.\n",
    "    2. Synonyms of the word if the exist\n",
    "\n",
    "    Output Format (comma seperated json format):\n",
    "\n",
    "        explanation: <Simple explanation like to a child>\n",
    "        synonyms: <comman seperated synoyms>\n",
    "\n",
    "    German Word: {word}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '\\n'\n",
      "             '        You are a fluent German speaker that is great at '\n",
      "             'following instructions. \\n'\n",
      "             '    ',\n",
      "  'role': 'system'},\n",
      " {'content': '\\n'\n",
      "             '    Given a German word, supply the following\\n'\n",
      "             '    1. A explanation of the word in english in very simple terms '\n",
      "             'like to a child.\\n'\n",
      "             '    2. Synonyms of the word if the exist\\n'\n",
      "             '\\n'\n",
      "             '    Output Format (comma seperated json format):\\n'\n",
      "             '\\n'\n",
      "             '        explanation: <Simple explanation like to a child>\\n'\n",
      "             '        synonyms: <comman seperated synoyms>\\n'\n",
      "             '\\n'\n",
      "             '    German Word: Hilfreich\\n',\n",
      "  'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "german_word = \"Hilfreich\"\n",
    "temperature = 0.5\n",
    "\n",
    "messages = [\n",
    "    system_prompt(),\n",
    "    user_prompt(word=german_word)\n",
    "]\n",
    "\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Async = False\n",
      "{'messages': [{'assistant': '{\\n'\n",
      "                            '    \"explanation\": \"Helpful means something that '\n",
      "                            'is useful or assists in a positive way.\",\\n'\n",
      "                            '    \"synonyms\": \"nützlich, dienlich, förderlich\"\\n'\n",
      "                            '}',\n",
      "               'finish_reason': 'stop'}],\n",
      " 'usage': {'completion_tokens': 41, 'prompt_tokens': 113, 'total_tokens': 154}}\n"
     ]
    }
   ],
   "source": [
    "is_async = False\n",
    "stream = False\n",
    "gpt = OpenAILLM(is_async)\n",
    "print(f\"Is Async = {gpt.is_async}\")\n",
    "response = gpt.prompt(messages=messages, temperature=temperature, stream=stream)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Streaming Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Async = True\n",
      "{'messages': [{'assistant': '{\\n'\n",
      "                            '    \"explanation\": \"Helpful means something that '\n",
      "                            'is useful or provides assistance.\",\\n'\n",
      "                            '    \"synonyms\": \"nützlich, dienlich, förderlich\"\\n'\n",
      "                            '}',\n",
      "               'finish_reason': 'stop'}],\n",
      " 'usage': {'completion_tokens': 38, 'prompt_tokens': 113, 'total_tokens': 151}}\n"
     ]
    }
   ],
   "source": [
    "is_async = True\n",
    "stream = False\n",
    "gpt = OpenAILLM(is_async)\n",
    "print(f\"Is Async = {gpt.is_async}\")\n",
    "response = await gpt.aprompt(messages=messages, temperature=temperature, stream=stream)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Async = False\n",
      "{'assistant': '', 'finish_reason': ''}\n",
      "{'no_role': '{\\n', 'finish_reason': ''}\n",
      "{'no_role': '   ', 'finish_reason': ''}\n",
      "{'no_role': ' \"', 'finish_reason': ''}\n",
      "{'no_role': 'ex', 'finish_reason': ''}\n",
      "{'no_role': 'planation', 'finish_reason': ''}\n",
      "{'no_role': '\":', 'finish_reason': ''}\n",
      "{'no_role': ' \"', 'finish_reason': ''}\n",
      "{'no_role': 'Help', 'finish_reason': ''}\n",
      "{'no_role': 'ful', 'finish_reason': ''}\n",
      "{'no_role': ' means', 'finish_reason': ''}\n",
      "{'no_role': ' something', 'finish_reason': ''}\n",
      "{'no_role': ' that', 'finish_reason': ''}\n",
      "{'no_role': ' is', 'finish_reason': ''}\n",
      "{'no_role': ' useful', 'finish_reason': ''}\n",
      "{'no_role': ' and', 'finish_reason': ''}\n",
      "{'no_role': ' can', 'finish_reason': ''}\n",
      "{'no_role': ' assist', 'finish_reason': ''}\n",
      "{'no_role': ' you', 'finish_reason': ''}\n",
      "{'no_role': ' in', 'finish_reason': ''}\n",
      "{'no_role': ' some', 'finish_reason': ''}\n",
      "{'no_role': ' way', 'finish_reason': ''}\n",
      "{'no_role': '.\",\\n', 'finish_reason': ''}\n",
      "{'no_role': '   ', 'finish_reason': ''}\n",
      "{'no_role': ' \"', 'finish_reason': ''}\n",
      "{'no_role': 'syn', 'finish_reason': ''}\n",
      "{'no_role': 'onyms', 'finish_reason': ''}\n",
      "{'no_role': '\":', 'finish_reason': ''}\n",
      "{'no_role': ' \"', 'finish_reason': ''}\n",
      "{'no_role': 'n', 'finish_reason': ''}\n",
      "{'no_role': 'üt', 'finish_reason': ''}\n",
      "{'no_role': 'z', 'finish_reason': ''}\n",
      "{'no_role': 'lich', 'finish_reason': ''}\n",
      "{'no_role': ',', 'finish_reason': ''}\n",
      "{'no_role': ' bra', 'finish_reason': ''}\n",
      "{'no_role': 'uch', 'finish_reason': ''}\n",
      "{'no_role': 'bar', 'finish_reason': ''}\n",
      "{'no_role': ',', 'finish_reason': ''}\n",
      "{'no_role': ' di', 'finish_reason': ''}\n",
      "{'no_role': 'en', 'finish_reason': ''}\n",
      "{'no_role': 'lich', 'finish_reason': ''}\n",
      "{'no_role': '\"\\n', 'finish_reason': ''}\n",
      "{'no_role': '}', 'finish_reason': ''}\n",
      "{'no_role': '', 'finish_reason': 'stop'}\n"
     ]
    }
   ],
   "source": [
    "is_async = False\n",
    "stream = True\n",
    "gpt = OpenAILLM(is_async)\n",
    "print(f\"Is Async = {gpt.is_async}\")\n",
    "response = gpt.prompt(messages=messages, temperature=temperature, stream=stream)\n",
    "for val in response:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Async = True\n"
     ]
    }
   ],
   "source": [
    "is_async = True\n",
    "stream = True\n",
    "gpt = OpenAILLM(is_async)\n",
    "print(f\"Is Async = {gpt.is_async}\")\n",
    "response = await gpt.aprompt(messages=messages, temperature=temperature, stream=stream)\n",
    "async for val in response:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "queryverse-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
