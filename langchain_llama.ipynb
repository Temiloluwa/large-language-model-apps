{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.pydantic_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m \u001b[39mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39msagemaker_endpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m SagemakerEndpoint\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msagemaker_endpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m LLMContentHandler\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquestion_answering\u001b[39;00m \u001b[39mimport\u001b[39;00m load_qa_chain\n",
      "File \u001b[0;32m~/codes-and-scripts/tutorials/llm_apps/sagemaker_endpoint.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m LLM\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m enforce_stop_tokens\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpydantic_v1\u001b[39;00m \u001b[39mimport\u001b[39;00m Extra, root_validator\n\u001b[1;32m     10\u001b[0m INPUT_TYPE \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mINPUT_TYPE\u001b[39m\u001b[39m\"\u001b[39m, bound\u001b[39m=\u001b[39mUnion[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]])\n\u001b[1;32m     11\u001b[0m OUTPUT_TYPE \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mOUTPUT_TYPE\u001b[39m\u001b[39m\"\u001b[39m, bound\u001b[39m=\u001b[39mUnion[\u001b[39mstr\u001b[39m, List[List[\u001b[39mfloat\u001b[39m]]])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.pydantic_v1'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from sagemaker_endpoint import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import json\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "example_doc_1 = \"\"\"\n",
    "Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.\n",
    "Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.\n",
    "Therefore, Peter stayed with her at the hospital for 3 days without leaving.\n",
    "\"\"\"\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=example_doc_1,\n",
    "    )\n",
    "]\n",
    "\n",
    "query = \"\"\"How long was Elizabeth hospitalized?\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({prompt: prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "SagemakerEndpoint.model_kwargs = {'custom_attributes':'accept_eula=true'}\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "llm = SagemakerEndpoint(\n",
    "        endpoint_name=\"meta-textgeneration-llama-2-7b-2023-08-26-18-08-09-987\",\n",
    "        credentials_profile_name=\"temmie\",\n",
    "        region_name=\"us-east-1\",\n",
    "        model_kwargs={\"temperature\": 1e-10},\n",
    "        content_handler=content_handler\n",
    "        #custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "#llm._endpoint_kwargs = {\"custom_attributes\":\"accept_eula=true\"}\n",
    "chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    ")\n",
    "\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Result: [\n",
      "  {\n",
      "    \"generation\":\" to make a difference.\\nWhen I was younger, I dreamed of being a doctor. I thought that I would be able to make a difference in people's lives. I thought that I would be able to help people who were sick and injured.\\nNow that I am older, I know that I don\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "# Specify the AWS configuration profile to use\n",
    "profile_name = 'temmie'\n",
    "\n",
    "# Create a session with the specified profile\n",
    "session = boto3.Session(profile_name=profile_name)\n",
    "\n",
    "# Create a SageMaker runtime client using the session\n",
    "sagemaker_runtime = session.client('sagemaker-runtime')\n",
    "  \n",
    "\n",
    "# Specify the endpoint name and payload\n",
    "endpoint_name = 'meta-textgeneration-llama-2-7b-2023-08-26-18-08-09-987'\n",
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    }\n",
    "}\n",
    "payload_json = json.dumps(payload)\n",
    "\n",
    "# Specify the custom attributes\n",
    "custom_attributes = 'accept_eula=true'\n",
    "\n",
    "# Perform inference using the invoke_endpoint method\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',  # Content type for the payload\n",
    "    CustomAttributes=custom_attributes,\n",
    "    Body=payload_json.encode('utf-8')\n",
    ")\n",
    "\n",
    "# Parse and print the inference response\n",
    "inference_result = response['Body'].read()\n",
    "print(\"Inference Result:\", inference_result.decode('utf-8'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__config__': langchain.llms.sagemaker_endpoint.Config,\n",
       "              '__fields__': {'cache': ModelField(name='cache', type=Optional[bool], required=False, default=None),\n",
       "               'verbose': ModelField(name='verbose', type=bool, required=False, default_factory='<function _get_verbosity>'),\n",
       "               'callbacks': ModelField(name='callbacks', type=Union[List[langchain.callbacks.base.BaseCallbackHandler], BaseCallbackManager, NoneType], required=False, default=None),\n",
       "               'callback_manager': ModelField(name='callback_manager', type=Optional[BaseCallbackManager], required=False, default=None),\n",
       "               'tags': ModelField(name='tags', type=Optional[List[str]], required=False, default=None),\n",
       "               'client': ModelField(name='client', type=Optional[Any], required=False, default=None),\n",
       "               'endpoint_name': ModelField(name='endpoint_name', type=str, required=False, default=''),\n",
       "               'region_name': ModelField(name='region_name', type=str, required=False, default=''),\n",
       "               'credentials_profile_name': ModelField(name='credentials_profile_name', type=Optional[str], required=False, default=None),\n",
       "               'content_handler': ModelField(name='content_handler', type=LLMContentHandler, required=True),\n",
       "               'model_kwargs': ModelField(name='model_kwargs', type=Optional[Mapping[Any, Any]], required=False, default=None),\n",
       "               'endpoint_kwargs': ModelField(name='endpoint_kwargs', type=Optional[Mapping[Any, Any]], required=False, default=None)},\n",
       "              '__exclude_fields__': {'callbacks': True,\n",
       "               'callback_manager': True,\n",
       "               'tags': True},\n",
       "              '__include_fields__': None,\n",
       "              '__validators__': {'verbose': [<pydantic.class_validators.Validator at 0x11e455090>]},\n",
       "              '__pre_root_validators__': [],\n",
       "              '__post_root_validators__': [(False,\n",
       "                <function langchain.llms.base.BaseLLM.raise_deprecation(cls, values: Dict) -> Dict>),\n",
       "               (False,\n",
       "                <function langchain.llms.sagemaker_endpoint.SagemakerEndpoint.validate_environment(cls, values: Dict) -> Dict>)],\n",
       "              '__schema_cache__': {},\n",
       "              '__json_encoder__': <staticmethod at 0x1292f5ee0>,\n",
       "              '__custom_root_type__': False,\n",
       "              '__private_attributes__': {'_lc_kwargs': ModelPrivateAttr(default=PydanticUndefined, default_factory=<class 'dict'>)},\n",
       "              '__slots__': set(),\n",
       "              '__hash__': None,\n",
       "              '__class_vars__': set(),\n",
       "              '__module__': 'langchain.llms.sagemaker_endpoint',\n",
       "              '__annotations__': {'client': typing.Any,\n",
       "               'endpoint_name': str,\n",
       "               'region_name': str,\n",
       "               'credentials_profile_name': typing.Optional[str],\n",
       "               'content_handler': langchain.llms.sagemaker_endpoint.LLMContentHandler,\n",
       "               'model_kwargs': typing.Optional[typing.Dict],\n",
       "               'endpoint_kwargs': typing.Optional[typing.Dict]},\n",
       "              '__doc__': 'Wrapper around custom Sagemaker Inference Endpoints.\\n\\n    To use, you must supply the endpoint name from your deployed\\n    Sagemaker model & the region where it is deployed.\\n\\n    To authenticate, the AWS client uses the following methods to\\n    automatically load credentials:\\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\n\\n    If a specific credential profile should be used, you must pass\\n    the name of the profile from the ~/.aws/credentials file that is to be used.\\n\\n    Make sure the credentials / roles used have the required policies to\\n    access the Sagemaker endpoint.\\n    See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\\n    ',\n",
       "              'Config': langchain.llms.sagemaker_endpoint.SagemakerEndpoint.Config,\n",
       "              'validate_environment': <classmethod at 0x1292f5e20>,\n",
       "              '_identifying_params': <property at 0x1293073b0>,\n",
       "              '_llm_type': <property at 0x129307400>,\n",
       "              '_call': <function langchain.llms.sagemaker_endpoint.SagemakerEndpoint._call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[langchain.callbacks.manager.CallbackManagerForLLMRun] = None, **kwargs: Any) -> str>,\n",
       "              '__abstractmethods__': frozenset(),\n",
       "              '_abc_impl': <_abc._abc_data at 0x12930ba40>,\n",
       "              '__signature__': <pydantic.utils.ClassAttribute at 0x1292f5f10>,\n",
       "              'model_kwargs': {'custom_attributes': 'accept_eula=true'}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SagemakerEndpoint.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Result: [\n",
      "  {\n",
      "    \"generation\":\"\\n2016: Who is the president of Nigeia?\\n2016: Who is the president of Nigeia? - The answer is\\n2016: Who is the president of Nigeia? - The answer is Goodluck Jonathan\\n2016: Who\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def perform_sagemaker_inference(profile_name, endpoint_name, payload, custom_attributes):\n",
    "    # Specify the AWS configuration profile to use\n",
    "    \n",
    "\n",
    "    # Create a session with the specified profile\n",
    "    session = boto3.Session(profile_name=profile_name)\n",
    "\n",
    "    # Create a SageMaker runtime client using the session\n",
    "    sagemaker_runtime = session.client('sagemaker-runtime')\n",
    "\n",
    "    # Convert payload to JSON string\n",
    "    payload_json = json.dumps(payload)\n",
    "\n",
    "    # Perform inference using the invoke_endpoint method\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',  # Content type for the payload\n",
    "        CustomAttributes=custom_attributes,\n",
    "        Body=payload_json.encode('utf-8')\n",
    "    )\n",
    "\n",
    "    # Parse and return the inference response\n",
    "    inference_result = response['Body'].read()\n",
    "    return inference_result.decode('utf-8')\n",
    "\n",
    "# Specify the endpoint name and payload\n",
    "endpoint_name = 'meta-textgeneration-llama-2-7b-2023-08-26-18-08-09-987'\n",
    "payload = {\n",
    "    \"inputs\": \"Who is the president of Nigeia?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    }\n",
    "}\n",
    "custom_attributes = 'accept_eula=true'\n",
    "profile_name = 'temmie'\n",
    "\n",
    "# Call the function and print the result\n",
    "result = perform_sagemaker_inference(profile_name, endpoint_name, payload, custom_attributes)\n",
    "print(\"Inference Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallLama(SagemakerEndpoint):\n",
    "    def __init__(self, profile_name, endpoint_name, payload, custom_attributes):\n",
    "        super(CallLama, self).__init__(credentials_profile_name=profile_name)\n",
    "        self.profile_name = profile_name\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.payload = payload\n",
    "        self.custom_attributes = custom_attributes\n",
    "\n",
    "    def _call(self):\n",
    "        session = boto3.Session(profile_name=self.profile_name)\n",
    "\n",
    "        # Create a SageMaker runtime client using the session\n",
    "        sagemaker_runtime = session.client('sagemaker-runtime')\n",
    "\n",
    "        # Convert payload to JSON string\n",
    "        payload_json = json.dumps(self.payload)\n",
    "\n",
    "        # Perform inference using the invoke_endpoint method\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=self.endpoint_name,\n",
    "            ContentType='application/json',  # Content type for the payload\n",
    "            CustomAttributes=self.custom_attributes,\n",
    "            Body=payload_json.encode('utf-8')\n",
    "        )\n",
    "\n",
    "        # Parse and return the inference response\n",
    "        inference_result = response['Body'].read()\n",
    "        return inference_result.decode('utf-8')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for CallLama\ncontent_handler\n  field required (type=value_error.missing)\n__root__\n  Could not load credentials to authenticate with AWS client. Please check that credentials in the specified profile name are valid. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[39m=\u001b[39m load_qa_chain(\n\u001b[0;32m----> 2\u001b[0m     llm\u001b[39m=\u001b[39mCallLama(profile_name, endpoint_name, payload, custom_attributes),\n\u001b[1;32m      3\u001b[0m     prompt\u001b[39m=\u001b[39mPROMPT,\n\u001b[1;32m      4\u001b[0m )\n",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mCallLama.__init__\u001b[0;34m(self, profile_name, endpoint_name, payload, custom_attributes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, profile_name, endpoint_name, payload, custom_attributes):\n\u001b[0;32m----> 3\u001b[0m     \u001b[39msuper\u001b[39;49m(CallLama, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(credentials_profile_name\u001b[39m=\u001b[39;49mprofile_name)\n\u001b[1;32m      4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofile_name \u001b[39m=\u001b[39m profile_name\n\u001b[1;32m      5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint_name \u001b[39m=\u001b[39m endpoint_name\n",
      "File \u001b[0;32m~/miniconda3/envs/naija_highlights/lib/python3.9/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/naija_highlights/lib/python3.9/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for CallLama\ncontent_handler\n  field required (type=value_error.missing)\n__root__\n  Could not load credentials to authenticate with AWS client. Please check that credentials in the specified profile name are valid. (type=value_error)"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(\n",
    "    llm=CallLama(profile_name, endpoint_name, payload, custom_attributes),\n",
    "    prompt=PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naija_highlights",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
